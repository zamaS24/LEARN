{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\monms\\\\Desktop\\\\TF_developer\\\\C3_Notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index:  {'i': 1, 'and': 2, 'love': 3, 'my': 4, 'dog': 5, 'his': 6, 'he': 7, 'loves': 8, 'cat': 9, 'you': 10, 'or': 11, 'laptop': 12, 'for': 13, 'sure': 14, 'can': 15, 'this': 16, 'is': 17, 'not': 18, 'him': 19}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[8, 6, 9], [8, 6, 2, 6, 12]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'i love my dog', \n",
    "    'he loves his cat', \n",
    "    'i and You and i or i love my dog and his laptop for sure can this is not him'\n",
    "]\n",
    "\n",
    "\n",
    "test_data = [\n",
    "    'Hamza loves his cat',\n",
    "    'Hamza loves his car and his laptop'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('word index: ', word_index)\n",
    "\n",
    "tokenizer.texts_to_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  8,  6,  9],\n",
       "       [ 8,  6,  2,  6, 12]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exï¿½cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!cd public_data_train\n",
    "!wget -r -np -nH --cut-dirs=2 https://finfo.usthb.dz/wp-content/uploads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 13 5511k   13  724k    0     0   808k      0  0:00:06 --:--:--  0:00:06  809k\n",
      " 30 5511k   30 1661k    0     0   877k      0  0:00:06  0:00:01  0:00:05  877k\n",
      " 49 5511k   49 2701k    0     0   931k      0  0:00:05  0:00:02  0:00:03  932k\n",
      " 69 5511k   69 3853k    0     0   989k      0  0:00:05  0:00:03  0:00:02  990k\n",
      " 89 5511k   89 4957k    0     0  1012k      0  0:00:05  0:00:04  0:00:01 1013k\n",
      "100 5511k  100 5511k    0     0  1026k      0  0:00:05  0:00:05 --:--:-- 1070k\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Downlaod the dataset \n",
    "!curl https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json -o sarcasm.json\n",
    "\n",
    "# in the colab notebook we had I guess \n",
    "\n",
    "\n",
    "import json \n",
    "\n",
    "with open('sarcasm.json', 'r') as f: \n",
    "    datastore = json.load(f)\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore: \n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Person thinks time or. make'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['']\n",
    "\n",
    "\n",
    "stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \n",
    "    \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \n",
    "    \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \n",
    "    \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \n",
    "    \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \n",
    "    \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \n",
    "    \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \n",
    "    \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
    "    \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \n",
    "    \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \n",
    "    \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \n",
    "    \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \n",
    "    \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \n",
    "    \"yourselves\" \n",
    "]\n",
    "\n",
    "\n",
    "sentence = 'a Person who thinks all the time after any or. make it '\n",
    "\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "words = [word for word in words if word not in stopwords]\n",
    "\n",
    "sentence = ' '.join(words)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
