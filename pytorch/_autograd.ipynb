{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# auto grad package in pytorch: \n",
    "\"\"\"\n",
    "Methods: \n",
    "        - torch.tensor(..., requires_grad = True)\n",
    "        - W.grad : to get the gradients\n",
    "        - W.grad.zero_() : resets the gradients to zero \n",
    "        - torch.softmax(tensor, dim): dim is the axis you want to apply to \n",
    "\n",
    "Infos: \n",
    "        y = x +2, and x has required_grad => y will have grad_function\n",
    "def int(){\n",
    "        print('soemthing to do here right ')\n",
    "        print('this is another thing to do here right?')\n",
    "}\n",
    "\n",
    "\n",
    "Super infos: \n",
    "        a tensor has \n",
    "                .data\n",
    "                .grad (the actual dl/dw)\n",
    "                .grad_fn\n",
    "                .is_leaf\n",
    "                .requires_grad\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 5., 6.], dtype=torch.float16, grad_fn=<AddBackward0>)\n",
      "z tensor(43., dtype=torch.float16, grad_fn=<MeanBackward0>)\n",
      "x.grad tensor([3., 4., 5., 6.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4], dtype=torch.float16, requires_grad=True)\n",
    "\n",
    "# this will create a computational graph \n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y * y*2 # this tensor will have also the grad thing \n",
    "z = z.mean()\n",
    "# you will see the AddBackward function , so that we can do the back propagation later\n",
    "print('z' , z)\n",
    "\n",
    "z.backward() # dz/dx\n",
    "print('x.grad', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5., 10.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "y = torch.pow(x, 2) +1 \n",
    "print(y)\n",
    "\n",
    "y[0].backward() # this only works for scalar outpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad # I guess this actually returns dy/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.detach>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.detach \n",
    "y # here same but doens't require the gradient \n",
    "\n",
    "with torch.no_grad() : # this disables the autograd\n",
    "    y = x + 2 \n",
    "    # now y will not have grad function \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighs = torch.ones(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n",
      "tensor([ 2.,  4.,  6.,  8., 10., 12., 14., 16., 18.])\n"
     ]
    }
   ],
   "source": [
    "weighs = torch.tensor([1,2,3,4,5,6,7,8,9], dtype=torch.float32, requires_grad=True)\n",
    "# require_grad will say that weighs are considered parameters of df/dw\n",
    "# whatever the F function that will use the weighs \n",
    "\n",
    "for epoch in range(2): \n",
    "    model_output =  torch.pow(weighs, 2).sum()\n",
    "\n",
    "    model_output.backward() # this will calculate do/dw\n",
    "\n",
    "    print(weighs.grad)# ici normalemnet on aura le\n",
    "\n",
    "    weighs.grad.zero_() # set gadients to zero, so that in the next step it will not accumulate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "weights = torch.ones(4, dtype=torch.float32,  requires_grad=True) # we need to specify this \n",
    "optimizer = torch.optim.SGD([weights ], lr=0.01)\n",
    "\n",
    "optimizer.step() # update the weights \n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "weights.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch is amazing, and then you're going to add up on hugging face you will put it together\n",
    "# how is this going to be optimized for the research ?\n",
    "\n",
    "x = torch.tensor(5)\n",
    "x\n",
    "\n",
    "import torchvision\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
