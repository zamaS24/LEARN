{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    - you need to reset the gradients to zero at the end because they are cumulative I guess\n",
    "\n",
    "    VERY VERY IMPORTANT: \n",
    "        - making W = W - lr *W.grad or W -= lr*W.grad is NOT THE SAME THING.\n",
    "            the fist one creates a new tensor, hence he new tensor does not carry over the gradient information from the old tensor.\n",
    "            the W-= modifies directly the tensor\n",
    "    methods: \n",
    "        torch.randn(batch_size, input_dim, requires_grad=True) : random tensor\n",
    "'''\n",
    "\n",
    "\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x= torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "\n",
    "# forward pass and compute loss\n",
    "y_hat = w * x \n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "print('loss: ', loss)\n",
    "\n",
    "# backward pass \n",
    "loss.backward()\n",
    "\n",
    "w.grad\n",
    "print(w.grad)   # this should be the dloss/dw ?\n",
    "\n",
    "# updata weights \n",
    "\n",
    "# newt forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a simple perceptron \n",
    "# I'm keeping everything global, functions will be justr procedures. \n",
    "\n",
    "\n",
    "\n",
    "# you should never like make a shape of this \n",
    "\n",
    "\n",
    "# X = torch.tensor([[1.0, 2.0, 3.0]], dtype=torch.float16,)\n",
    "# Y = torch.tensor([[3.0, 5.0, 7.0]], dtype=torch.float16)\n",
    "\n",
    "# W = torch.zeros((3,1), dtype=torch.float16, requires_grad=True)\n",
    "# B = torch.tensor((1,1), dtype=torch.float16, requires_grad=True)\n",
    "\n",
    "X = torch.tensor([[1.0, 2.0, 3.0]], dtype=torch.float32)  # Making X a 2D tensor\n",
    "Y = torch.tensor([[3.0, 5.0, 7.0]], dtype=torch.float32)  # Making Y a 2D tensor\n",
    "\n",
    "W = torch.zeros((3, 1), dtype=torch.float32, requires_grad=True)\n",
    "B = torch.zeros((1, 1), dtype=torch.float32, requires_grad=True)  # Making B a 2D tensor\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "\n",
    "def forward(): \n",
    "    return torch.matmul(X, W) + B\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(10): \n",
    "\n",
    "    y_hat = forward()\n",
    "    \n",
    "    l = ((Y - y_hat)**2).mean()\n",
    "    l.backward()\n",
    "\n",
    "\n",
    "    # update the parameters\n",
    "    with torch.no_grad(): \n",
    "        W -= lr *W.grad\n",
    "        B -= lr *B.grad\n",
    "\n",
    "    # resets the gradient to zero \n",
    "    W.grad.zero_()\n",
    "    B.grad.zero_()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 27.66666603088379\n",
      "Epoch 2, Loss: 14.916666984558105\n",
      "Epoch 3, Loss: 8.66916561126709\n",
      "Epoch 4, Loss: 5.6078925132751465\n",
      "Epoch 5, Loss: 4.1078667640686035\n",
      "Epoch 6, Loss: 3.3728549480438232\n",
      "Epoch 7, Loss: 3.0126988887786865\n",
      "Epoch 8, Loss: 2.8362224102020264\n",
      "Epoch 9, Loss: 2.749748945236206\n",
      "Epoch 10, Loss: 2.7073771953582764\n",
      "Trained weights: tensor([[0.3239],\n",
      "        [0.6478],\n",
      "        [0.9718]], requires_grad=True)\n",
      "Trained bias: tensor([[0.3239]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define input, output, weights, and bias\n",
    "X = torch.tensor([[1.0, 2.0, 3.0]], dtype=torch.float32)  # Making X a 2D tensor\n",
    "Y = torch.tensor([[3.0, 5.0, 7.0]], dtype=torch.float32)  # Making Y a 2D tensor\n",
    "\n",
    "W = torch.zeros((3, 1), dtype=torch.float32, requires_grad=True)\n",
    "B = torch.zeros((1, 1), dtype=torch.float32, requires_grad=True)  # Making B a 2D tensor\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "def forward(): \n",
    "    return torch.matmul(X, W) + B\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(10): \n",
    "    # Forward pass\n",
    "    y_hat = forward()\n",
    "    \n",
    "    # Compute loss\n",
    "    l= ((y_hat - Y)**2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad\n",
    "        B -= lr * B.grad\n",
    "\n",
    "\n",
    "    # Zero gradients\n",
    "    W.grad.zero_()\n",
    "    B.grad.zero_()\n",
    "    \n",
    "    # Print loss for monitoring\n",
    "    print(f'Epoch {epoch+1}, Loss: {l.item()}')\n",
    "\n",
    "print(\"Trained weights:\", W)\n",
    "print(\"Trained bias:\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event loop and synchronization algorithms in python and you gotta make it everyday better you feel me bruh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
