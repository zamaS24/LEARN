{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "outputs are :  tensor([0.0321, 0.0871, 0.2369, 0.6439])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "    [1,2,3,4], \n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print('outputs are : ', outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 =  0.2872765362262726\n",
      "l2 =  1.0455279350280762\n"
     ]
    }
   ],
   "source": [
    "# cross entropy  = -1/n Sum(yi.log(yi_^))\n",
    "\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "y_true = torch.tensor([0,2,1])\n",
    "\n",
    "good = torch.tensor([0])\n",
    "\n",
    "\n",
    "y_pred_good = torch.tensor([\n",
    "    [4.0, 2.0, 0.1],\n",
    "    [0.3, 0.7, 2], \n",
    "    [0.2,2.2,0.9]\n",
    "])\n",
    "\n",
    "\n",
    "y_pred_bad = torch.tensor([\n",
    "    [2.0, 0.2, 0.1],\n",
    "    [0.3, 0.7, 0.1], \n",
    "    [0.2,0.1,0.9]\n",
    "])\n",
    "\n",
    "\n",
    "# batchsize stuff \n",
    "l1 = loss(y_pred_good, y_true)\n",
    "l2 = loss(y_pred_bad, y_true)\n",
    "\n",
    "\n",
    "print('l1 = ', l1.item())\n",
    "print('l2 = ', l2.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> :warning: The y_hat should not be one hot encoded \n",
    "> becauset nn.CrossEntropyLoss uses LogSoftmax and NLLLoss (negative likelihood loos)\n",
    "> no softmax in last layer \n",
    "> Y has class label on hot encoded \n",
    "> Y_pred has raw scores (logits), no softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n",
      "tensor([4.0000, 2.0000, 2.2000])\n"
     ]
    }
   ],
   "source": [
    "_, prediction1 = torch.max(y_pred_good, 1) # returns actual values and their indices\n",
    "# this means apply the max function every time in the dimension 1, \n",
    "_, prediction2 = torch.max(y_pred_bad, 1)\n",
    "\n",
    "print(prediction1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn \n",
    "import numpy \n",
    "import torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module): \n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes) -> None:\n",
    "        super(NeuralNet2, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.relu(x)\n",
    "        out = self.linear2(x) \n",
    "        # no softmax here \n",
    "\n",
    "        return out \n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() # applies softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with binary cross entropy we need to add the sigmoid function at the end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will just change the model where the output size is 1, and we add the sigmoid function, \n",
    "# the criterion will be the binary cross entropy loss\n",
    "# nn.BCELoss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
