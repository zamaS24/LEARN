{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "1. design the model (input, output, forward pass)\n",
    "2. Construct the loss and optimizer \n",
    "3. Training loop   \n",
    "    3.1 forward pass (compute prediction)  \n",
    "    3.2 compute gradients (backwad pass)  \n",
    "    3.3 update the weights  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    - optimizer from torch.nn.optim\n",
    "    - losses from torch.nn \n",
    "    - Activation functions from torch.nn.functional \n",
    "\n",
    "\n",
    "        nn.MSELoss()\n",
    "        torch.optim.SGD()\n",
    "    METHODS: \n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        optimier.zero_grad() : set the gradients to zero \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss:30.0, w:0.300\n",
      "epoch:2, loss:21.674999237060547, w:0.555\n",
      "epoch:3, loss:15.660187721252441, w:0.772\n",
      "epoch:4, loss:11.314486503601074, w:0.956\n",
      "epoch:5, loss:8.17471694946289, w:1.113\n",
      "epoch:6, loss:5.9062323570251465, w:1.246\n",
      "epoch:7, loss:4.2672529220581055, w:1.359\n",
      "epoch:8, loss:3.083089828491211, w:1.455\n",
      "epoch:9, loss:2.227532148361206, w:1.537\n",
      "epoch:10, loss:1.609391689300537, w:1.606\n",
      "epoch:11, loss:1.1627856492996216, w:1.665\n",
      "epoch:12, loss:0.8401124477386475, w:1.716\n",
      "epoch:13, loss:0.6069811582565308, w:1.758\n",
      "epoch:14, loss:0.4385439455509186, w:1.794\n",
      "epoch:15, loss:0.3168478012084961, w:1.825\n",
      "epoch:16, loss:0.22892260551452637, w:1.851\n",
      "epoch:17, loss:0.1653965264558792, w:1.874\n",
      "epoch:18, loss:0.11949898302555084, w:1.893\n",
      "epoch:19, loss:0.08633805811405182, w:1.909\n",
      "epoch:20, loss:0.0623791441321373, w:1.922\n",
      "epoch:21, loss:0.0450688973069191, w:1.934\n",
      "epoch:22, loss:0.03256231173872948, w:1.944\n",
      "epoch:23, loss:0.02352631464600563, w:1.952\n",
      "epoch:24, loss:0.016997724771499634, w:1.960\n",
      "epoch:25, loss:0.012280836701393127, w:1.966\n",
      "epoch:26, loss:0.008872910402715206, w:1.971\n",
      "epoch:27, loss:0.0064106592908501625, w:1.975\n",
      "epoch:28, loss:0.004631685093045235, w:1.979\n",
      "epoch:29, loss:0.0033464201260358095, w:1.982\n",
      "epoch:30, loss:0.0024177832528948784, w:1.985\n",
      "epoch:31, loss:0.0017468547448515892, w:1.987\n",
      "epoch:32, loss:0.0012621148489415646, w:1.989\n",
      "epoch:33, loss:0.0009118800517171621, w:1.991\n",
      "epoch:34, loss:0.0006588209653273225, w:1.992\n",
      "epoch:35, loss:0.0004760062729474157, w:1.993\n",
      "epoch:36, loss:0.00034391897497698665, w:1.994\n",
      "epoch:37, loss:0.0002484779979567975, w:1.995\n",
      "epoch:38, loss:0.00017952272901311517, w:1.996\n",
      "epoch:39, loss:0.00012970640091225505, w:1.996\n",
      "epoch:40, loss:9.371075429953635e-05, w:1.997\n",
      "epoch:41, loss:6.770494655938819e-05, w:1.997\n",
      "epoch:42, loss:4.891453863820061e-05, w:1.998\n",
      "epoch:43, loss:3.5343608033144847e-05, w:1.998\n",
      "epoch:44, loss:2.5532886866130866e-05, w:1.998\n",
      "epoch:45, loss:1.844714643084444e-05, w:1.999\n",
      "epoch:46, loss:1.3328777640708722e-05, w:1.999\n",
      "epoch:47, loss:9.631531611375976e-06, w:1.999\n",
      "epoch:48, loss:6.958316589589231e-06, w:1.999\n",
      "epoch:49, loss:5.027383849665057e-06, w:1.999\n",
      "epoch:50, loss:3.632284688137588e-06, w:1.999\n",
      "epoch:51, loss:2.6243997126584873e-06, w:1.999\n",
      "epoch:52, loss:1.8964256014442071e-06, w:2.000\n",
      "epoch:53, loss:1.3698846714760293e-06, w:2.000\n",
      "epoch:54, loss:9.894590675685322e-07, w:2.000\n",
      "epoch:55, loss:7.148483405217121e-07, w:2.000\n",
      "epoch:56, loss:5.168862458049261e-07, w:2.000\n",
      "epoch:57, loss:3.7350218917708844e-07, w:2.000\n",
      "epoch:58, loss:2.697535705920018e-07, w:2.000\n",
      "epoch:59, loss:1.9482058633002453e-07, w:2.000\n",
      "epoch:60, loss:1.4073337695208465e-07, w:2.000\n",
      "epoch:61, loss:1.0175587306093803e-07, w:2.000\n",
      "epoch:62, loss:7.338856278238381e-08, w:2.000\n",
      "epoch:63, loss:5.315412465733971e-08, w:2.000\n",
      "epoch:64, loss:3.836930773104541e-08, w:2.000\n",
      "epoch:65, loss:2.7700096438820765e-08, w:2.000\n",
      "epoch:66, loss:2.0093764874218323e-08, w:2.000\n",
      "epoch:67, loss:1.4520100677373193e-08, w:2.000\n",
      "epoch:68, loss:1.0521901572246861e-08, w:2.000\n",
      "epoch:69, loss:7.592394268840508e-09, w:2.000\n",
      "epoch:70, loss:5.487198251330483e-09, w:2.000\n",
      "epoch:71, loss:3.9741685498029256e-09, w:2.000\n",
      "epoch:72, loss:2.866613613150548e-09, w:2.000\n",
      "epoch:73, loss:2.0563000191486935e-09, w:2.000\n",
      "epoch:74, loss:1.479023126194079e-09, w:2.000\n",
      "epoch:75, loss:1.0658141036401503e-09, w:2.000\n",
      "epoch:76, loss:7.718661265698756e-10, w:2.000\n",
      "epoch:77, loss:5.525180313270539e-10, w:2.000\n",
      "epoch:78, loss:3.978932738846197e-10, w:2.000\n",
      "epoch:79, loss:2.8819613362429664e-10, w:2.000\n",
      "epoch:80, loss:2.063416104647331e-10, w:2.000\n",
      "epoch:81, loss:1.4670220593870908e-10, w:2.000\n",
      "epoch:82, loss:1.0176748332924035e-10, w:2.000\n",
      "epoch:83, loss:7.317169092857512e-11, w:2.000\n",
      "epoch:84, loss:5.0661697059695143e-11, w:2.000\n",
      "epoch:85, loss:3.807443249570497e-11, w:2.000\n",
      "epoch:86, loss:2.7284841053187847e-11, w:2.000\n",
      "epoch:87, loss:2.0307311388023663e-11, w:2.000\n",
      "epoch:88, loss:1.5347723092418164e-11, w:2.000\n",
      "epoch:89, loss:1.1098677532572765e-11, w:2.000\n",
      "epoch:90, loss:6.821210263296962e-12, w:2.000\n",
      "epoch:91, loss:5.076827847005916e-12, w:2.000\n",
      "epoch:92, loss:3.595346242946107e-12, w:2.000\n",
      "epoch:93, loss:2.7746693831431912e-12, w:2.000\n",
      "epoch:94, loss:1.7053025658242404e-12, w:2.000\n",
      "epoch:95, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:96, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:97, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:98, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:99, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:100, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:101, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:102, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:103, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:104, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:105, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:106, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:107, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:108, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:109, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:110, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:111, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:112, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:113, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:114, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:115, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:116, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:117, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:118, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:119, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:120, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:121, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:122, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:123, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:124, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:125, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:126, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:127, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:128, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:129, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:130, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:131, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:132, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:133, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:134, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:135, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:136, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:137, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:138, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:139, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:140, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:141, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:142, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:143, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:144, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:145, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:146, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:147, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:148, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:149, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:150, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:151, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:152, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:153, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:154, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:155, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:156, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:157, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:158, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:159, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:160, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:161, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:162, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:163, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:164, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:165, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:166, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:167, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:168, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:169, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:170, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:171, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:172, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:173, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:174, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:175, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:176, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:177, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:178, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:179, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:180, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:181, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:182, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:183, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:184, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:185, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:186, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:187, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:188, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:189, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:190, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:191, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:192, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:193, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:194, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:195, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:196, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:197, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:198, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:199, loss:8.988365607365267e-13, w:2.000\n",
      "epoch:200, loss:8.988365607365267e-13, w:2.000\n",
      "prediction after training: 10.000\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0], dtype=torch.float32)\n",
    "Y = torch.tensor([2.0, 4.0, 6.0, 8.0], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# return Y_pred\n",
    "def forward(X):\n",
    "    return W * X; \n",
    "\n",
    "\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "loss = nn.MSELoss() # this returns a callable function \n",
    "\n",
    "optimizer = torch.optim.SGD([W], lr=lr) # for updating the weighs\n",
    "\n",
    "\n",
    "# training loops\n",
    "for epoch in range(epochs) : \n",
    "\n",
    "    # predict\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # calculate loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    # gradient bacward pass\n",
    "    l.backward() # puts in w.grads the dl/dw\n",
    "\n",
    "    # calculate the gradients \n",
    "    optimizer.step()\n",
    "\n",
    "    # now empty the gradients \n",
    "    # W.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'epoch:{epoch+1}, loss:{l}, w:{W:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "print(f'prediction after training: {forward(5):.3f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's make it with a model \n",
    "few things change:   \n",
    "- we  need to define the input and output shapes  \n",
    "- we need to convert teh X array to be a 2d array   \n",
    "- when we predict, we need to pass a tensor as input   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape  torch.Size([4, 1])\n",
      "n samples : 4\n",
      "n_features:  1\n",
      "prediction before training:  -0.31068968772888184\n",
      "epoch:1, loss:30.16092300415039, w:2.000\n",
      "epoch:2, loss:20.982641220092773, w:2.000\n",
      "epoch:3, loss:14.613698959350586, w:2.000\n",
      "epoch:4, loss:10.194101333618164, w:2.000\n",
      "epoch:5, loss:7.127113342285156, w:2.000\n",
      "epoch:6, loss:4.998673439025879, w:2.000\n",
      "epoch:7, loss:3.5214760303497314, w:2.000\n",
      "epoch:8, loss:2.4961628913879395, w:2.000\n",
      "epoch:9, loss:1.7844040393829346, w:2.000\n",
      "epoch:10, loss:1.2902166843414307, w:2.000\n",
      "epoch:11, loss:0.9469995498657227, w:2.000\n",
      "epoch:12, loss:0.7085397243499756, w:2.000\n",
      "epoch:13, loss:0.5427693128585815, w:2.000\n",
      "epoch:14, loss:0.42743945121765137, w:2.000\n",
      "epoch:15, loss:0.3471103012561798, w:2.000\n",
      "epoch:16, loss:0.29106947779655457, w:2.000\n",
      "epoch:17, loss:0.2518841326236725, w:2.000\n",
      "epoch:18, loss:0.2243955135345459, w:2.000\n",
      "epoch:19, loss:0.20502489805221558, w:2.000\n",
      "epoch:20, loss:0.1912895143032074, w:2.000\n",
      "epoch:21, loss:0.18146544694900513, w:2.000\n",
      "epoch:22, loss:0.17435748875141144, w:2.000\n",
      "epoch:23, loss:0.1691356897354126, w:2.000\n",
      "epoch:24, loss:0.16522470116615295, w:2.000\n",
      "epoch:25, loss:0.16222456097602844, w:2.000\n",
      "epoch:26, loss:0.1598585695028305, w:2.000\n",
      "epoch:27, loss:0.15793393552303314, w:2.000\n",
      "epoch:28, loss:0.15631747245788574, w:2.000\n",
      "epoch:29, loss:0.15491633117198944, w:2.000\n",
      "epoch:30, loss:0.15366658568382263, w:2.000\n",
      "epoch:31, loss:0.15252313017845154, w:2.000\n",
      "epoch:32, loss:0.15145552158355713, w:2.000\n",
      "epoch:33, loss:0.15044181048870087, w:2.000\n",
      "epoch:34, loss:0.14946724474430084, w:2.000\n",
      "epoch:35, loss:0.14852158725261688, w:2.000\n",
      "epoch:36, loss:0.1475975513458252, w:2.000\n",
      "epoch:37, loss:0.14668995141983032, w:2.000\n",
      "epoch:38, loss:0.1457955241203308, w:2.000\n",
      "epoch:39, loss:0.1449117809534073, w:2.000\n",
      "epoch:40, loss:0.14403703808784485, w:2.000\n",
      "epoch:41, loss:0.14316999912261963, w:2.000\n",
      "epoch:42, loss:0.14230994880199432, w:2.000\n",
      "epoch:43, loss:0.14145632088184357, w:2.000\n",
      "epoch:44, loss:0.14060863852500916, w:2.000\n",
      "epoch:45, loss:0.1397666484117508, w:2.000\n",
      "epoch:46, loss:0.1389300376176834, w:2.000\n",
      "epoch:47, loss:0.13809876143932343, w:2.000\n",
      "epoch:48, loss:0.1372726559638977, w:2.000\n",
      "epoch:49, loss:0.1364516019821167, w:2.000\n",
      "epoch:50, loss:0.13563556969165802, w:2.000\n",
      "epoch:51, loss:0.1348244994878769, w:2.000\n",
      "epoch:52, loss:0.13401831686496735, w:2.000\n",
      "epoch:53, loss:0.1332169473171234, w:2.000\n",
      "epoch:54, loss:0.13242042064666748, w:2.000\n",
      "epoch:55, loss:0.1316285878419876, w:2.000\n",
      "epoch:56, loss:0.13084162771701813, w:2.000\n",
      "epoch:57, loss:0.13005927205085754, w:2.000\n",
      "epoch:58, loss:0.1292816698551178, w:2.000\n",
      "epoch:59, loss:0.12850871682167053, w:2.000\n",
      "epoch:60, loss:0.12774035334587097, w:2.000\n",
      "epoch:61, loss:0.1269766241312027, w:2.000\n",
      "epoch:62, loss:0.12621751427650452, w:2.000\n",
      "epoch:63, loss:0.12546275556087494, w:2.000\n",
      "epoch:64, loss:0.12471268326044083, w:2.000\n",
      "epoch:65, loss:0.12396697700023651, w:2.000\n",
      "epoch:66, loss:0.12322588264942169, w:2.000\n",
      "epoch:67, loss:0.12248914688825607, w:2.000\n",
      "epoch:68, loss:0.12175677716732025, w:2.000\n",
      "epoch:69, loss:0.12102887779474258, w:2.000\n",
      "epoch:70, loss:0.12030526250600815, w:2.000\n",
      "epoch:71, loss:0.11958594620227814, w:2.000\n",
      "epoch:72, loss:0.11887092143297195, w:2.000\n",
      "epoch:73, loss:0.11816022545099258, w:2.000\n",
      "epoch:74, loss:0.11745379865169525, w:2.000\n",
      "epoch:75, loss:0.11675155162811279, w:2.000\n",
      "epoch:76, loss:0.11605354398488998, w:2.000\n",
      "epoch:77, loss:0.11535963416099548, w:2.000\n",
      "epoch:78, loss:0.11466985195875168, w:2.000\n",
      "epoch:79, loss:0.11398430913686752, w:2.000\n",
      "epoch:80, loss:0.11330285668373108, w:2.000\n",
      "epoch:81, loss:0.11262541264295578, w:2.000\n",
      "epoch:82, loss:0.111952044069767, w:2.000\n",
      "epoch:83, loss:0.11128267645835876, w:2.000\n",
      "epoch:84, loss:0.11061736196279526, w:2.000\n",
      "epoch:85, loss:0.10995595157146454, w:2.000\n",
      "epoch:86, loss:0.10929854959249496, w:2.000\n",
      "epoch:87, loss:0.10864503681659698, w:2.000\n",
      "epoch:88, loss:0.10799546539783478, w:2.000\n",
      "epoch:89, loss:0.10734982788562775, w:2.000\n",
      "epoch:90, loss:0.10670801997184753, w:2.000\n",
      "epoch:91, loss:0.10606999695301056, w:2.000\n",
      "epoch:92, loss:0.10543577373027802, w:2.000\n",
      "epoch:93, loss:0.10480542480945587, w:2.000\n",
      "epoch:94, loss:0.10417884588241577, w:2.000\n",
      "epoch:95, loss:0.10355588793754578, w:2.000\n",
      "epoch:96, loss:0.10293680429458618, w:2.000\n",
      "epoch:97, loss:0.10232139378786087, w:2.000\n",
      "epoch:98, loss:0.10170961916446686, w:2.000\n",
      "epoch:99, loss:0.10110149532556534, w:2.000\n",
      "epoch:100, loss:0.1004970371723175, w:2.000\n",
      "epoch:101, loss:0.09989617764949799, w:2.000\n",
      "epoch:102, loss:0.09929890185594559, w:2.000\n",
      "epoch:103, loss:0.09870525449514389, w:2.000\n",
      "epoch:104, loss:0.09811508655548096, w:2.000\n",
      "epoch:105, loss:0.09752839803695679, w:2.000\n",
      "epoch:106, loss:0.09694528579711914, w:2.000\n",
      "epoch:107, loss:0.09636573493480682, w:2.000\n",
      "epoch:108, loss:0.09578955173492432, w:2.000\n",
      "epoch:109, loss:0.09521691501140594, w:2.000\n",
      "epoch:110, loss:0.0946476012468338, w:2.000\n",
      "epoch:111, loss:0.09408166259527206, w:2.000\n",
      "epoch:112, loss:0.0935191661119461, w:2.000\n",
      "epoch:113, loss:0.09296000003814697, w:2.000\n",
      "epoch:114, loss:0.09240425378084183, w:2.000\n",
      "epoch:115, loss:0.09185180068016052, w:2.000\n",
      "epoch:116, loss:0.09130260348320007, w:2.000\n",
      "epoch:117, loss:0.09075669944286346, w:2.000\n",
      "epoch:118, loss:0.09021411836147308, w:2.000\n",
      "epoch:119, loss:0.0896746963262558, w:2.000\n",
      "epoch:120, loss:0.08913858234882355, w:2.000\n",
      "epoch:121, loss:0.08860556781291962, w:2.000\n",
      "epoch:122, loss:0.08807583153247833, w:2.000\n",
      "epoch:123, loss:0.08754925429821014, w:2.000\n",
      "epoch:124, loss:0.08702585101127625, w:2.000\n",
      "epoch:125, loss:0.08650551736354828, w:2.000\n",
      "epoch:126, loss:0.08598832786083221, w:2.000\n",
      "epoch:127, loss:0.08547419309616089, w:2.000\n",
      "epoch:128, loss:0.08496318012475967, w:2.000\n",
      "epoch:129, loss:0.08445516228675842, w:2.000\n",
      "epoch:130, loss:0.08395020663738251, w:2.000\n",
      "epoch:131, loss:0.08344827592372894, w:2.000\n",
      "epoch:132, loss:0.08294937759637833, w:2.000\n",
      "epoch:133, loss:0.0824534147977829, w:2.000\n",
      "epoch:134, loss:0.08196045458316803, w:2.000\n",
      "epoch:135, loss:0.08147041499614716, w:2.000\n",
      "epoch:136, loss:0.08098334819078445, w:2.000\n",
      "epoch:137, loss:0.08049911260604858, w:2.000\n",
      "epoch:138, loss:0.08001788705587387, w:2.000\n",
      "epoch:139, loss:0.0795394629240036, w:2.000\n",
      "epoch:140, loss:0.07906389981508255, w:2.000\n",
      "epoch:141, loss:0.07859119772911072, w:2.000\n",
      "epoch:142, loss:0.07812131941318512, w:2.000\n",
      "epoch:143, loss:0.07765425741672516, w:2.000\n",
      "epoch:144, loss:0.07718995213508606, w:2.000\n",
      "epoch:145, loss:0.0767284482717514, w:2.000\n",
      "epoch:146, loss:0.07626966387033463, w:2.000\n",
      "epoch:147, loss:0.0758136585354805, w:2.000\n",
      "epoch:148, loss:0.07536041736602783, w:2.000\n",
      "epoch:149, loss:0.07490985095500946, w:2.000\n",
      "epoch:150, loss:0.0744619220495224, w:2.000\n",
      "epoch:151, loss:0.0740167647600174, w:2.000\n",
      "epoch:152, loss:0.07357420772314072, w:2.000\n",
      "epoch:153, loss:0.07313436269760132, w:2.000\n",
      "epoch:154, loss:0.07269708067178726, w:2.000\n",
      "epoch:155, loss:0.07226242125034332, w:2.000\n",
      "epoch:156, loss:0.07183032482862473, w:2.000\n",
      "epoch:157, loss:0.07140093296766281, w:2.000\n",
      "epoch:158, loss:0.07097401469945908, w:2.000\n",
      "epoch:159, loss:0.07054967433214188, w:2.000\n",
      "epoch:160, loss:0.07012784481048584, w:2.000\n",
      "epoch:161, loss:0.06970857083797455, w:2.000\n",
      "epoch:162, loss:0.06929183006286621, w:2.000\n",
      "epoch:163, loss:0.06887751072645187, w:2.000\n",
      "epoch:164, loss:0.0684657096862793, w:2.000\n",
      "epoch:165, loss:0.0680563747882843, w:2.000\n",
      "epoch:166, loss:0.0676494687795639, w:2.000\n",
      "epoch:167, loss:0.0672450140118599, w:2.000\n",
      "epoch:168, loss:0.06684296578168869, w:2.000\n",
      "epoch:169, loss:0.06644333153963089, w:2.000\n",
      "epoch:170, loss:0.06604605168104172, w:2.000\n",
      "epoch:171, loss:0.06565115600824356, w:2.000\n",
      "epoch:172, loss:0.0652586817741394, w:2.000\n",
      "epoch:173, loss:0.0648684948682785, w:2.000\n",
      "epoch:174, loss:0.06448070704936981, w:2.000\n",
      "epoch:175, loss:0.06409511715173721, w:2.000\n",
      "epoch:176, loss:0.06371195614337921, w:2.000\n",
      "epoch:177, loss:0.06333103030920029, w:2.000\n",
      "epoch:178, loss:0.06295231729745865, w:2.000\n",
      "epoch:179, loss:0.06257601827383041, w:2.000\n",
      "epoch:180, loss:0.0622018501162529, w:2.000\n",
      "epoch:181, loss:0.061829935759305954, w:2.000\n",
      "epoch:182, loss:0.06146024167537689, w:2.000\n",
      "epoch:183, loss:0.061092808842659, w:2.000\n",
      "epoch:184, loss:0.060727592557668686, w:2.000\n",
      "epoch:185, loss:0.06036445498466492, w:2.000\n",
      "epoch:186, loss:0.060003578662872314, w:2.000\n",
      "epoch:187, loss:0.059644825756549835, w:2.000\n",
      "epoch:188, loss:0.05928822606801987, w:2.000\n",
      "epoch:189, loss:0.058933746069669724, w:2.000\n",
      "epoch:190, loss:0.058581359684467316, w:2.000\n",
      "epoch:191, loss:0.0582311674952507, w:2.000\n",
      "epoch:192, loss:0.05788295343518257, w:2.000\n",
      "epoch:193, loss:0.05753690376877785, w:2.000\n",
      "epoch:194, loss:0.057192929089069366, w:2.000\n",
      "epoch:195, loss:0.05685090646147728, w:2.000\n",
      "epoch:196, loss:0.05651101842522621, w:2.000\n",
      "epoch:197, loss:0.05617322027683258, w:2.000\n",
      "epoch:198, loss:0.05583730340003967, w:2.000\n",
      "epoch:199, loss:0.05550345778465271, w:2.000\n",
      "epoch:200, loss:0.0551716610789299, w:2.000\n",
      "prediction after training: 9.598526\n"
     ]
    }
   ],
   "source": [
    "# training examples \n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "# X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "# Y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# we won't need this \n",
    "# W = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "n_samples , n_features = X.shape\n",
    "print('X.shape ', X.shape)\n",
    "print('n samples :', n_samples)\n",
    "print('n_features: ', n_features)\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# create a linear model \n",
    "model = nn.Linear(\n",
    "    input_size, # 1\n",
    "    output_size #1\n",
    ")\n",
    "\n",
    "\n",
    "epochs = 200\n",
    "lr = 0.01\n",
    "loss = nn.MSELoss() # this is a callable function \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr) # for updating the weighs\n",
    "\n",
    "\n",
    "print('prediction before training: ', model(x_test).item())\n",
    "# training loops\n",
    "for epoch in range(epochs) : \n",
    "\n",
    "    # predict\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # calculate loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    # gradient bacward pass\n",
    "    l.backward() # puts in w.grads the dl/dw\n",
    "\n",
    "    # calculate the gradients \n",
    "    optimizer.step()\n",
    "\n",
    "    # now empty the gradients \n",
    "    # W.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f'epoch:{epoch+1}, loss:{l}, w:{W:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "print(f'prediction after training: {model(x_test).item():3f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module): \n",
    "\n",
    "    def __init__(self, input_dim, output_dim): \n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        # define the layer\n",
    "        self.linear = nn.Linear(\n",
    "            input_dim, \n",
    "            output_dim\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X): \n",
    "        return self.linear(X)\n",
    "    \n",
    "\n",
    "\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "model = LinearRegression(input_size, output_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
