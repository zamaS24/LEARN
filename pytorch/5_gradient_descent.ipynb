{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First in numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "METHODS\n",
    "    - with torch.no_grad():     this should not be in the computation graph \n",
    "        # do something \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3335.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \tloss: 30.0 \tw:1.200\n",
      "epoch: 1 \tloss: 4.799999237060547 \tw:1.680\n",
      "epoch: 2 \tloss: 0.7680001854896545 \tw:1.872\n",
      "epoch: 3 \tloss: 0.1228799968957901 \tw:1.949\n",
      "epoch: 4 \tloss: 0.019660834223031998 \tw:1.980\n",
      "epoch: 5 \tloss: 0.0031457357108592987 \tw:1.992\n",
      "epoch: 6 \tloss: 0.0005033080233260989 \tw:1.997\n",
      "epoch: 7 \tloss: 8.053186320466921e-05 \tw:1.999\n",
      "epoch: 8 \tloss: 1.2884394891443662e-05 \tw:1.999\n",
      "epoch: 9 \tloss: 2.0613531432900345e-06 \tw:2.000\n",
      "prediction after training: 9.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "X = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)\n",
    "Y = np.array([2.0, 4.0, 6.0, 8.0], dtype=np.float32)\n",
    "\n",
    "W = 0.0\n",
    "\n",
    "\n",
    "# return Y_pred \n",
    "def forward(X):\n",
    "    return W * X; \n",
    "\n",
    "# MAE loss function \n",
    "def loss(y, y_pred): \n",
    "    return ((y_pred-y)**2).mean() \n",
    "\n",
    "#thsmlqksjfd qdslfkjqsdf i print somethin gfabulous in the real time of it okay ? thank you so umch for this one too mate okay. I happen to do so with this one   \n",
    "# don't worry it\n",
    "# insert something in the real time of it okay? something happening right        \n",
    "\n",
    "# something is happening right in here right?\n",
    "# also you have something in the mean time of it to do it here too right ? \n",
    "# something in the real time is happening this time\n",
    "\n",
    "def gradient(x, y, y_pred): \n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "\n",
    "print('prediction before training: f(5) = ', forward(5))\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "# liek this is the rela new topic for this one too mate okay.\n",
    "# I happen to do so right  something happening in this time right ? \n",
    "# can you add something ?  \n",
    "# now you can do something about it right\n",
    "\n",
    "for epoch in tqdm(range(epochs)) : \n",
    "\n",
    "    # predict\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # this one is just for information\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    # gradient \n",
    "    dw = gradient(X,Y,y_pred)\n",
    "\n",
    "\n",
    "    # update the wieghs\n",
    "    W = W -lr*dw\n",
    "\n",
    "     \n",
    "    print(f'epoch: {epoch} \\tloss: {l:.11f} \\tw:{W:.3f}')\n",
    "\n",
    "\n",
    "print(f'prediction after training: {forward(5):.3f}' )\n",
    "# somethign in the mean time right ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) =  tensor(0., grad_fn=<MulBackward0>)\n",
      "epoch: 1 \tloss: 30.0 \tw:0.300\n",
      "epoch: 2 \tloss: 21.674999237060547 \tw:0.555\n",
      "epoch: 3 \tloss: 15.660187721252441 \tw:0.772\n",
      "epoch: 4 \tloss: 11.314486503601074 \tw:0.956\n",
      "epoch: 5 \tloss: 8.17471694946289 \tw:1.113\n",
      "epoch: 6 \tloss: 5.9062323570251465 \tw:1.246\n",
      "epoch: 7 \tloss: 4.2672529220581055 \tw:1.359\n",
      "epoch: 8 \tloss: 3.083089828491211 \tw:1.455\n",
      "epoch: 9 \tloss: 2.227532148361206 \tw:1.537\n",
      "epoch: 10 \tloss: 1.609391689300537 \tw:1.606\n",
      "prediction after training: 8.031\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0], dtype=torch.float32)\n",
    "Y = torch.tensor([2.0, 4.0, 6.0, 8.0], dtype=torch.float32)\n",
    "\n",
    "W = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# return Y_pred\n",
    "def forward(X):\n",
    "    return W * X; \n",
    "\n",
    "# MAE loss function \n",
    "def loss(y, y_pred): \n",
    "    return ((y_pred-y)**2).mean() \n",
    "\n",
    "def another_func(y, y_pred): \n",
    "    return ((y_pred-y)**2).mean() \n",
    "\n",
    "\n",
    "print('prediction before training: f(5) = ', forward(5)) # do something about if if you say so right ?\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(epochs) : \n",
    "\n",
    "    # predict\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # this one is just for information\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    # gradient = bacward\n",
    "    # dw = gradient(X,Y,y_pred)\n",
    "    l.backward() # puts in w.grads the dl/dw\n",
    "\n",
    "\n",
    "    # update the wieghs\n",
    "\n",
    "    # this should not be in the computational graph \n",
    "    with torch.no_grad(): \n",
    "        W = W - lr * W.grad \n",
    "\n",
    "    # now empty the gradients \n",
    "    W.grad.zero_()\n",
    "\n",
    "    print(f'epoch:{epoch+1}, loss:{l}, w:{W:.3f}')\n",
    "\n",
    "\n",
    "\n",
    "print(f'prediction after training: {forward(5):.3f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### :warning: ***you can see that the results are not the same, because the gradient computation in backward are not the same as the gradient in one we did before where we knew exactly what the function of the gradient is*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "\n",
    "population = 100\n",
    "num_inside = 0\n",
    "num_outside = 0\n",
    "\n",
    "\n",
    "\n",
    "for _ in range(population):\n",
    "    x = random.randint(0,1)\n",
    "    y = random.randint(0,1)\n",
    "\n",
    "\n",
    "    if x**2 + y**2 > 1: \n",
    "        num_outside += 1\n",
    "    else: \n",
    "        num_inside += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
